%Set document class
\documentclass{article}

%Load math symbol packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz} 
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{indentfirst}
\usepackage{graphicx}

%User defined commands
\newcommand{\var}{\operatorname{Var}}

\begin{document}
\begin{center}
	\huge{\bf Math 181A: Homework 2} \\
	Merrick Qiu 
\end{center}

\subsection*{Problem 1: 5.2.17}
For the MOM estimator, we have that
\begin{align*}
	\mu_1 &= E[y_i] \\
	&= \int_0^\theta y\frac{2y}{\theta^2} \,dy \\
	&= \left[\frac{2y^3}{3\theta^2}\right]_0^\theta \\
	&= \frac{2}{3}\theta.
\end{align*}
Solving out for the parameter yields
\[
	\theta = \frac{3}{2}\mu_1.
\].
Replacing the mean by the sample mean yields 
\[
	\hat{\theta} 
	= \frac{3}{2}\hat{\mu_1}
	=\frac{3}{2}\bar{Y}.
\]
For the given random sample, 
so the mean is $50$ $\theta_e = 75$.

For the MLE,
\[
	L(\theta) = \prod_{i=1}^n \frac{2y_i}{\theta^2} 
	=\frac{2^n}{\theta^{2n}} \prod_{i=1}^n y_i 
\]
$L(\theta)$ is maximized when $\theta$ is minimized,
so $\theta_e = y_max = 92$.
Compared to the MOM estimator, the MLE overestimates the parameter.
\newpage

\subsection*{Problem 2: 5.2.18}
We have that 
\begin{align*}
	\mu_1 &= E[y_i] \\
	&= \int_0^1 y(\theta^2 + \theta)y^{\theta-1}(1-y) \,dy \\
	&= (\theta^2 + \theta) \int_0^1 y^{\theta}-y^{\theta+1} \,dy \\
	&= (\theta^2 + \theta) \left[\frac{y^{\theta+1}}{\theta+1} - 
		\frac{y^{\theta+2}}{\theta+2}\right]_0^1 \,dy \\
	&= (\theta^2 + \theta)\left(\frac{1}{\theta+1} - \frac{1}{\theta+2}\right) \\
	&= \frac{\theta}{\theta+2}. 
\end{align*}

Then we can solve out for $\theta_e$ in terms of $\bar{y}$.
\begin{align*}
	\bar{y} = \frac{\theta_e}{\theta_e+2}
	&\implies \theta_e\bar{y} + 2\bar{y} = \theta_e \\
	&\implies \theta_e(\bar{y}-1) = -2\bar{y} \\
	&\implies \theta_e = \frac{2\bar{y}}{1-\bar{y}}.
\end{align*}
\newpage

\subsection*{Problem 3: 5.2.21}

We have that 
\begin{align*}
	\mu_1 &= E[y_i] \\
	&= \int_{\theta_1-\theta_2}^{\theta_1+\theta_2} \frac{y}{2\theta_2} \,dy\\
	&= \left[\frac{y^2}{4\theta_2}\right]_{\theta_1-\theta_2}^{\theta_1+\theta_2} \\
	&= \theta_1.
\end{align*}
\begin{align*}
	\mu_2 &= E[y_i^2] \\
	&= \int_{\theta_1-\theta_2}^{\theta_1+\theta_2} \frac{y^2}{2\theta_2} \,dy\\
	&= \left[\frac{y^3}{6\theta_2}\right]_{\theta_1-\theta_2}^{\theta_1+\theta_2} \\
	&= \frac{1}{3}(3\theta_1^2+\theta_2^2).
\end{align*}

Thus we have that 
\[
	\theta_{1e} = \bar{y}
\]
\[
	\theta_{2e} = \sqrt{3\hat{\mu}_2-3\bar{y}^2}
	= \sqrt{\frac{3}{n}\sum_{i=1}^n y_i^2 -3\bar{y}^2}
\]
\newpage

\subsection*{Problem 4: 5.2.4}
We have that 
\[
	L(\theta) = \prod_{i=1}^{n} \frac{\theta^{2k_i}e^{-\theta^2}}{k_i!} \\
	= \frac{\theta^{2\sum_{i=1}^nk_i}}{e^{\theta^2n}} \prod_{i=1}^{n} \frac{1}{k_i!} \\
\]
\[
	\ln L(\theta) = \left(2\sum_{i=1}^nk_i \right)\ln \theta -n\theta^2
		+ \ln \prod_{i=1}^{n} \frac{1}{k_i!} 
\]
\[
	\frac{d}{d\theta} \ln L(\theta) = 
	\frac{1}{\theta}\left(2\sum_{i=1}^nk_i \right) -2n\theta 
	= \frac{2\sum_{i=1}^nk_i -2n\theta^2}{\theta}= 0
\]
This implies that 
\[
	\theta_e = \sqrt{\frac{1}{n} \left(\sum_{i=1}^nk_i \right)}
\]
\newpage

\subsection*{Problem 5: 5.2.6}

We have that 
\[
	L(\theta) 
	= \prod_{i=1}^n \frac{\theta}{2\sqrt{y_i}}e^{-\theta\sqrt{y_i}}
	= \frac{\theta^ne^{-\theta(\sum_{i=1}^n \sqrt{y_i})}}{2^n\prod_{i=1}^{n} \sqrt{y_i}}.
\]
\[
	\ln L(\theta) 
	= n\ln\frac{\theta}{2}-\theta\sum_{i=1}^n \sqrt{y_i} - \ln \prod_{i=1}^{n} \sqrt{y_i}.
\]
\[
	\frac{d}{d\theta} \ln L(\theta) 
	= \frac{n}{\theta} - \sum_{i=1}^n \sqrt{y_i} = 0.
\]
This implies that
\[
	\theta_e 
	= \frac{n}{\sum_{i=1}^n \sqrt{y_i}}
	\approx\frac{4}{8.77}
	\approx 0.456.
\]
\newpage

\subsection*{Problem 6: Uniform Distribution}
\begin{enumerate}
	\item We have that 
	\[
		\mu_1 = E[X_i] = \frac{a+b}{2}
	\]
	\begin{align*}
		\mu_2 &= E[X_i^2] \\
		&= \int_a^b \frac{x^2}{b-a} \\
		&= \left[\frac{x^3}{3(b-a)}\right]_a^b \\
		&= \frac{b^3-a^3}{3(b-a)}  \\
		&= \frac{a^2+ab+b^2}{3} 
	\end{align*}
	This implies that 
	\[
		a+b = 2\bar{x} \implies b = 2\bar{x} - a
	\]
	\begin{align*}
		\mu_2 = \frac{a^2+ab+b^2}{3} 
		&\implies \mu_2 = \frac{a^2+a(2\bar{x} - a)+(2\bar{x} - a)^2}{3} \\
		&\implies a^2-2\bar{x}a+4\bar{x}^2-3\mu_2 =0\\
		&\implies a = \frac{2\bar{x} \pm \sqrt{4\bar{x}^2-(16\bar{x}^2 -12\mu_2)}}{2} \\
		&\implies a = \bar{x} \pm \sqrt{3\mu_2-3\bar{x}} \\
	\end{align*}

	Since $a$ is the lower bound and $b$ is the upper bound,
	we have that 
	\[
		\hat{a} = \bar{x} - \sqrt{\frac{3}{n}\sum_{i=1}^n x_i^2 -3\bar{x}}
	\]
	\[
		\hat{b} = \bar{x} + \sqrt{\frac{3}{n}\sum_{i=1}^n x_i^2-3\bar{x}}
	\]
	\item The likelyhood function is
	\[	
		L(a, b) 
		= \prod_{i=1}^n f_X(x_i;a, b)
		= \begin{cases}
			\frac{1}{(b-a)^n} & a \leq x_i \leq b \text{ for all i} \\
			0 & \text{otherwise}
		\end{cases}
	\]
	The likelyhood function is maximized when $b-a$ is minimized.
	Thus $\hat{b} = \max(x_1,\hdots,x_n)$ and $\hat{a} = \min(x_1,\hdots,x_n)$.

	
		
	
\end{enumerate}
\end{document}

