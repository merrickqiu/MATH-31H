%Set document class
\documentclass{article}

%Load math symbol packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz} 
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{indentfirst}
\usepackage{graphicx}

%User defined commands
\newcommand{\var}{\operatorname{Var}}
\newcommand{\cov}{\operatorname{Cov}}

\begin{document}
\begin{center}
	\huge{\bf Math 181A: Homework 4} \\
	Merrick Qiu 
\end{center}

\subsection*{Problem 1: 5.4.19}
\begin{enumerate}
	\item $\hat{\theta_1} = Y_1$ is unbiased because 
		\begin{align*}
			E[Y_1] 
			&= \int_0^\infty \frac{1}{\theta}ye^{-y/\theta} \,dy \\
			&= \left[-e^{-y/\theta}(y+\theta)\right]_0^\infty \\
			&= 0 - (-\theta) \\
			&= \theta.
		\end{align*}
		$\hat{\theta_2} = \bar{Y}$ is unbiased because 
		\[
			E[\bar{Y}] = E[Y_1] = \theta.
		\]
		The pdf of $Y_{min}$ is 
		\[
			f_{Y_{min}}(y;\theta) 
			= nf_Y(y)(1-F_Y(y))^{n-1}
			= \frac{n}{\theta}e^{-ny/\theta}
		\]
		We have that $nY_{min}$ and $Y_1$ have the same pdf since
		\[
			f_{nY_{min}}
			=\frac{1}{n}f_{Y_{min}}(\frac{y}{n})
			=\frac{1}{\theta}e^{-y/\theta}.
		\]
		Thus $E[nY_{min}] = \theta$ and it is also unbiased.
	\item The variance of an exponential random variable is $\frac{1}{\lambda^2}$.
	In this case, $\lambda = \frac{1}{\theta}$ so $\var(Y_1) = \theta^2$.
	The variance of the sample is $\var(\bar{Y}) = \frac{\theta^2}{n}$.
	The variance of $nY_{min}$ is the same as $Y_1$ so it is also 
	$\var(nY_{min}) = \theta^2$.
	\item Since $\hat{\theta_1}$ has the same variance as $\hat{\theta_1}$,
	the relative efficiency of $\hat{\theta_1}$ to $\hat{\theta_3}$ is 1.
	The relative efficiency of  $\hat{\theta_2}$ to $\hat{\theta_3}$ is 
	$\frac{\theta^2}{\theta^2/n} = n$.
\end{enumerate}
\newpage

\subsection*{Problem 2: 5.4.20}
The expected value of a Poisson distribution is $\lambda$,
so both $\hat{\lambda_1}$ and $\hat{\lambda_1}$ are unbiased.
 Since $\var(\hat{\lambda_2}) = \frac{\var(\hat{\lambda_1})}{n}$,
 the relative efficiency is 
 $\frac{\var(\hat{\lambda_1})/n}{\var(\hat{\lambda_1})} = \frac{1}{n}$.
\newpage

\subsection*{Problem 3: 5.4.22}

The variance of the estimator is 
\[
	\var(cW_1 + (1-c)W_2) = c^2\sigma_1^2 + (1-c)^2\sigma_2^2. 
\]
To minimize the variance, 
we can take the derivative with respect to $c$ yielding
\[
	\frac{\partial}{\partial c} (c^2\sigma_1^2 + (1-c)^2\sigma_2^2)
	= 2\sigma_1^2c -2\sigma_2^2(1-c) 
	= 0.
\]
\begin{align*}
	2\sigma_1^2c -2\sigma_2^2(1-c) = 0
	&\implies 2c(\sigma_1^2 + \sigma_2^2) = 2\sigma_2^2 \\
	&\implies c = \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2}.
\end{align*}
\newpage

\subsection*{Problem 4}

\begin{enumerate}
	\item $\hat{\theta}$ has no bias because 
	\begin{align*}
		E[c\hat{\theta_1} + (1-c)\hat{\theta_2}]
		&= cE[\hat{\theta_1}] + (1-c)E[\hat{\theta_2}] \\
		&= c\theta + (1-c)\theta \\
		&= \theta
	\end{align*}

	The variance of $\hat{\theta}$ is 
	\begin{align*}
		\var(c\hat{\theta_1} + (1-c)\hat{\theta_2}) 
		&= c^2\var(\hat{\theta_1}) + (1-c)^2\var(\hat{\theta_2}) 
		+2c(1-c)\cov(\hat{\theta_1}, \hat{\theta_2})\\
		&= c^2\sigma^2 + (1-c)^2\frac{\sigma^2}{2} + \frac{2c(1-c)\sigma^2}{3}
	\end{align*}

	For an unbiased variable, the mean squared error is the same as the variance.
	\item Taking the partial derivative with respect to $c$ yields
	\begin{align*}
		\frac{\partial}{\partial c} (c^2\sigma^2 + (1-c)^2\frac{\sigma^2}{2} + \frac{2c(1-c)\sigma^2}{3})
		&= 2\sigma^2c - \sigma^2(1-c) + \frac{2\sigma^2}{3}(1-2c)\\
		&= 0 
	\end{align*}
	\begin{align*}
		2\sigma^2c - \sigma^2(1-c) + \frac{2\sigma^2}{3}(1-2c) = 0
		&\implies (3\sigma^2 - \frac{4\sigma^2}{3})c = \sigma^2 - \frac{2\sigma^2}{3}\\
		&\implies c = \frac{\sigma^2/3}{5\sigma^2/3} = \frac{1}{5}.
	\end{align*}
\end{enumerate}
\newpage
\subsection*{Problem 5: 5.5.1}

The likelihood estimator yields $\hat{\theta} = \bar{Y}$ 
and therefore it has variance $\var(\hat{\theta}) = \frac{\theta^2}{n}$.
We have that
\[
	\log f_Y(y;\theta) = -\log \theta - \frac{y}{\theta}
\]
\[
	\frac{\partial}{\partial \theta} \log f_Y(y;\theta)
	= -\frac{1}{\theta} + \frac{y}{\theta^2}
\]
\[
	\frac{\partial^2}{\partial \theta^2} \log f_Y(y;\theta)
	= \frac{1}{\theta^2} - \frac{2y}{\theta^3}
\]
Since $E[Y] = \theta$, 
\[
	I(\theta) = -E\left[\frac{1}{\theta^2} - \frac{2Y}{\theta^3}\right]
	= \frac{1}{\theta^2}
\]
Therefore the Cramer-Rao lower bound is $\frac{\theta^2}{n}$
and so $\hat{\theta}$ is the best estimator.
\newpage
\subsection*{Problem 6: 5.5.2}

The variance of $\hat{\lambda}$ is $\frac{\lambda}{n}$ since it is a Poisson distribution.
We have that
\[
	\log f_X(x;\lambda) = -\lambda + x \log \lambda - \log x!
\]
\[
	\frac{\partial}{\partial \lambda} \log f_X(x;\theta)
	= -1 + x\frac{1}{\lambda}
\]
\[
	\frac{\partial^2}{\partial \lambda^2} \log f_X(x;\theta)
	= -x\frac{1}{\lambda^2}
\]
Since $E[X] = \lambda$,
\[
	I(\lambda) = - E[-X\frac{1}{\lambda^2}]
	= \frac{1}{\lambda}
\]
Therefore the Cramer-Rao lower bound is $\frac{\lambda}{n}$
and so $\hat{\lambda}$ is the best estimator.


\end{document}

