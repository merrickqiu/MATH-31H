{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00807139-dbe3-4bda-b929-5de1d17795f7",
   "metadata": {},
   "source": [
    "# CSE 152A Winter 2025 – Assignment 3\n",
    "\n",
    "- Assignment Published On: **Wednesday, February 19, 2025**\n",
    "\n",
    "- Due On: **Saturday, March 1, 2025 11:59 PM (Pacific Time)**\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Please answer the questions below using Python in the attached Jupyter notebook and follow the guidelines below:\n",
    " \n",
    "- This assignment must be completed **individually**. For more details, please follow the Academic Integrity Policy and Collaboration Policy posted on lecture slides.\n",
    "\n",
    "- All the solutions must be written in this Jupyter notebook.\n",
    "\n",
    "- After finishing the assignment in the notebook, please export the notebook as a PDF and submit both the notebook and the PDF (i.e. the `.ipynb` and the `.pdf` files) on Gradescope. (Note: Please ensure that all images/plots are clear in the pdf).\n",
    "\n",
    "- You may use basic algebra packages (e.g. `NumPy`, `SciPy`, etc) but you are not allowed to use open source codes that directly solve the problems. Feel free to ask the instructor and the teaching assistants if you are unsure about the packages to use.\n",
    "\n",
    "- It is highly recommended that you begin working on this assignment early.\n",
    "\n",
    "- Make sure that you read hints for questions (wherever given).\n",
    "\n",
    "**Late Policy:** Assignments submitted late will receive a 25% grade reduction for each 12 hours late (that is, 50% per day).\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "- You must submit both the `.ipynb` file and a `.pdf` version of your notebook.\n",
    "\n",
    "- Some methods to generate PDF\n",
    "    -  File -> Save and Export Notebook As -> `.html` -> Print -> Save as PDF\n",
    "    -  Using [nbconvert](https://nbconvert.readthedocs.io/en/latest/)\n",
    "\n",
    "\n",
    "- We will be grading primarily from your notebook.\n",
    "    - It is your responsibility to make sure that your code and outputs are visible.\n",
    " \n",
    "## Virtual Environment\n",
    "\n",
    "### Initial Set-up\n",
    "You can utilize a virtual environment (`venv`) in order to manage dependencies: [venv link](https://docs.python.org/3/library/venv.html) along with the libraries specified in `requirements.txt`.\n",
    "\n",
    "To create the environment:\n",
    "```\n",
    "python -m venv cse152a_hw3\n",
    "```\n",
    "\n",
    "To activate the environment (Mac/Linux):\n",
    "```\n",
    "source ./cse152a_hw3/bin/activate\n",
    "```\n",
    "\n",
    "To activate the environment (Windows):\n",
    "```\n",
    ".\\cse152a_hw3\\Scripts\\activate\n",
    "```\n",
    "\n",
    "Once the virtual environment is activated, you can install the libraries according to `requirements.txt` like so:\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "You should then add the environment to jupyter notebook like so:\n",
    "```\n",
    "python -m ipykernel install --user --name=cse152a_hw3\n",
    "```\n",
    "\n",
    "To deactivate the environment, simply:\n",
    "```\n",
    "deactivate\n",
    "```\n",
    "\n",
    "This assumes you have Python installed already.\n",
    "\n",
    "### Using the venv\n",
    "Once you've installed all the requirements within the venv, you can deactivate and would no longer need to repeat the above steps. You can simply open a Jupyter instance.\n",
    "\n",
    "Opening a Jupyter instance:\n",
    "```\n",
    "jupyter-lab\n",
    "```\n",
    "\n",
    "Ensure that you select the correct kernel (named `cse152a_hw3` if you followed the steps above) by clicking Kernel -> Change Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d7f689-1201-4fa4-b60b-01968cab9d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check that all libraries are installed\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b5905-9243-4a13-93d4-0d04656c57f4",
   "metadata": {},
   "source": [
    "## Problem 1: Image Classiﬁcation Using Bag-of-words [20 pts]\n",
    "\n",
    "We will now build a binary classifier to determine whether an input image contains an image with a human face or not (nonface). A training dataset has been provided consisting of images of human faces and other random objects (nonfaces). You are highly encouraged to look at the images yourself to get a feel for the data and the overall problem we will be working on. We will build a visual bag-of-words representation for images and build K-nearest neighbor classifier.\n",
    "\n",
    "## Review of bag-of-words for image classification\n",
    "\n",
    "We will attempt to solve image classification with the relatively simple but effective visual bag-of-words approach. We treat an image as a set of regions and ignore the spatial relationships between different regions. The image classification problem can then be solved by counting the frequencies of different regions appearing in an image. Image classification using bag-of-words includes the following steps: \n",
    "\n",
    "- **Feature Detection**: First, we need to perform feature detection to find interest points within our image to build our features descriptors around. Feature detection methods could be corner detection like you implemented in HW2 for example. In this assignment, you will be implementing a random sampler (uniformly sampling points from grids placed across the image). We've also implemented a SIFT keypoint feature detector to compare.\n",
    "\n",
    "- **Feature Extraction**: Second, we extract descriptors from our selected interest points. One commonly used feature descriptor are SIFT descriptors -- which has also been implemented for you to compare. We will extract features from every image in the training set. You will be implementing patch feature descriptors which simply means to flatten a window around the interest point. These features are collected to compute the visual vocabulary.\n",
    "\n",
    "- **Building visual vocabulary**: Once we have the features extracted from training images, we build a visual vocabulary by grouping them together to form clusters. We will use the k-means algorithm to group the features. Each cluster will be a visual \"word\" in our vocabulary. One reason why we need this step is to reduce the feature space and build a concise feature representation of images.\n",
    "\n",
    "- **Learning and recognition**: Given the visual vocabulary, each training image is represented by a histogram, where the features in the image populate bins that correspond to the visual word closest to them. Thereafter, we will use k-nearest neighbors to perform classification for a test image, also represented by a histogram using the same vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855eb4cf-7d8f-46bc-a977-76e1741d3b7e",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Below is some code used to load and process the data. We will build a classifier to predict whether an image is of a human face or not (nonface). The dataset contains 100 images with faces and 100 images without faces. We will pick 75 images from each group for training and use the other 25 from each for testing. Here, we denote faces as the \"positive class\" and nonfaces as the \"negative class\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42462b15-02ac-48bc-8154-97d971f5b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Paths and Setup Parameters\n",
    "pos_data = \"images/face/\"\n",
    "neg_data = \"images/nonface/\"\n",
    "train_ratio = 0.75\n",
    "im_size = (133, 200) # Resize each image to this size\n",
    "random.seed(1234)\n",
    "\n",
    "def list_image(data_root):\n",
    "    file_list = glob.glob(str(f\"{data_root}*.jpg\"))\n",
    "    im_num = len(file_list)\n",
    "    file_list = [file_list[i].replace('\\\\', '/') for i in range(im_num)]\n",
    "    return file_list\n",
    "\n",
    "def load_image(img_name, im_size):\n",
    "    # load the image, resize, and convert from color to grayscale\n",
    "    img = cv2.imread(img_name)\n",
    "    img = cv2.resize(img, (im_size[1], im_size[0]))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # img = img / 255.0 # Convert image range from [0,255] to [0.0, 1.0]\n",
    "    return img\n",
    "\n",
    "def create_split(img_list, ratio):\n",
    "    random.shuffle(img_list)\n",
    "    train_list = img_list[:round(len(img_list)*ratio)]\n",
    "    test_list = img_list[round(len(img_list)*ratio):]\n",
    "    return train_list, test_list\n",
    "\n",
    "# Data Preprocessing -- Load images and create training/testing splits\n",
    "pos_list = list_image(pos_data)\n",
    "neg_list = list_image(neg_data)\n",
    "train_pos_list, test_pos_list = create_split(pos_list, train_ratio)\n",
    "train_neg_list, test_neg_list = create_split(neg_list, train_ratio)\n",
    "train_list = train_pos_list + train_neg_list # Concatenate both positive and negative training data\n",
    "test_list = test_pos_list + test_neg_list\n",
    "# Build our labels: 1 for face (positive), 0 for nonface (negative)\n",
    "train_label = np.concatenate((np.ones(len(train_pos_list)), np.zeros(len(train_neg_list))))\n",
    "test_label = np.concatenate((np.ones(len(test_pos_list)), np.zeros(len(test_neg_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275e9d05-7ddc-48ed-8e11-376fc8791973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used to plot a training example from both the positive and negative training set\n",
    "sample_pos = load_image(pos_list[0], im_size)\n",
    "sample_neg = load_image(neg_list[0], im_size)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,10))\n",
    "axes[0].imshow(sample_pos, cmap=\"gray\")#, vmin=0.0, vmax=1.0)\n",
    "axes[0].axis(\"off\")\n",
    "axes[0].set_title(\"Sampled Positive Image\")\n",
    "axes[1].imshow(sample_neg, cmap=\"gray\")#, vmin=0.0, vmax=1.0)\n",
    "axes[1].axis(\"off\")\n",
    "axes[1].set_title(\"Sampled Negative Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb033154-d8ef-4ec9-9572-eadc66887491",
   "metadata": {},
   "source": [
    "## 1.1 Extract interest points from images [2 pts]\n",
    "\n",
    "You will now try two methods for this: \n",
    "\n",
    "- **(a) Uniformly sample the images:** You can divide the image into regular grids (chosen by the `w_grid` parameter), choose a random point in each grid uniformly and then randomly choose at maximum `n_pts` number of interest points (it's OK if your image ends up having less). If you cannot fit an entire grid (i.e. at right and bottom edges), then you can simply just ignore those grids in your algorithm.\n",
    "- **(b) Sample using SIFT keypoint detector:** This is already implemented for you. First, we use the SIFT keypoint detector to detect interest points within the image and then choose `n_pts` number of the detected interest points. We will compare this approach to approach (a) at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b223602-d826-458c-a7dc-54f0accd79d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters used for (1.1) -- DO NOT CHANGE\n",
    "n_pts = 200 # Number of points to be sampled\n",
    "w_grid = 5 # Width of the grid used for uniform sampling\n",
    "\n",
    "def uniform_sampling(im_size, n_pts, w_grid):\n",
    "    \"\"\"\n",
    "    Divide the image into grids and then randomly sample each grid uniformly.\n",
    "    Afterwards, randomly sample up to `n_pts` of them.\n",
    "    Note: we don't need the image itself, just the size to perform this.\n",
    "    Args:\n",
    "        im_size: size of images (height, width)\n",
    "        n_pts: maximum number of interest points to be extracted\n",
    "        w_grid: the width of the small grids\n",
    "    Return:\n",
    "        pts: a list of interest points [(x_0, y_0),...,(x_n, y_n)]\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "    \"\"\" END YOUR CODE \"\"\"\n",
    "    return pts\n",
    "\n",
    "def sift_keypoint_sampling(img, n_pts):\n",
    "    \"\"\"\n",
    "    Uses SIFT keypoint detector to extract interest points.\n",
    "    Args:\n",
    "        img: image from which you want to extract the interest points\n",
    "        n_pts: maximum number of interest points to be extracted\n",
    "    Return:\n",
    "        pts: a list of interest points [(x_0, y_0),...,(x_n, y_n)]\n",
    "    \"\"\"\n",
    "    sift = cv2.SIFT_create(n_pts)\n",
    "    keypoints = list(sift.detect(img, None))\n",
    "    random.shuffle(keypoints)\n",
    "    # keypoints = sorted(keypoints, key=lambda kp: kp.response, reverse=True)[:n_pts]\n",
    "    keypoints = keypoints[:n_pts]\n",
    "    pts = [(int(kp.pt[0]), int(kp.pt[1])) for kp in keypoints]\n",
    "    return pts\n",
    "\n",
    "def plot_interest_points(img, pts, title=None):\n",
    "    \"\"\"\n",
    "    Plots the images with their detected interest points.\n",
    "    Args:\n",
    "        img: grayscale img\n",
    "        pts: detected interest points [(y_0, x_0),...,(y_n, x_n)]\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(img, cmap=\"gray\")#, vmin=0.0, vmax=1.0)\n",
    "    x = [pts[i][0] for i in range(len(pts))]\n",
    "    y = [pts[i][1] for i in range(len(pts))]\n",
    "    plt.scatter(x,y, color=\"red\", s=1)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Here is code for you to test your implementation\n",
    "sample_img = load_image(train_list[0], im_size)\n",
    "pts_uniform = uniform_sampling(im_size, n_pts, w_grid)\n",
    "pts_sift = sift_keypoint_sampling(sample_img, n_pts)\n",
    "plot_interest_points(sample_img, pts_uniform, title=\"Uniform Sampling\")\n",
    "plot_interest_points(sample_img, pts_sift, title=\"SIFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b5beb-89ee-4a79-a74f-0e52a37b7a21",
   "metadata": {},
   "source": [
    "## 1.2. Extract features [2 pts]\n",
    "\n",
    "You are required to try two kinds of features: \n",
    "- **(a) SIFT feature:** Here we use the SIFT implementation to obtain the SIFT descriptors used as our features in the *OpenCV* package (this has already been implemented for you as an example).\n",
    "- **(b) Image Patch feature:** Extract a small image patch centered around each feature point. You should use the `patch_size` parameter to determine the size of your patches. You will flatten the patch into a 1D vector. If the patch goes outside the bounds of the image, you can discard the feature (it is OK that it decreases the total number of features extracted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c2014e-6b86-4e69-90e2-8d8a6816aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters used for (1.2) -- DO NOT CHANGE\n",
    "patch_size = 11 # Each patch will be 11x11 squares\n",
    "def extract_sift_feature(img, pts):\n",
    "    \"\"\"\n",
    "    Extracts a feature using OpenCV's SIFT descriptors\n",
    "    Args:\n",
    "        img: input image\n",
    "        pts: detected interest points in the previous step\n",
    "    Return:\n",
    "        features: a list of SIFT descriptor features for each interest point\n",
    "    \"\"\"\n",
    "    sift = cv2.SIFT_create()\n",
    "    kp = [cv2.KeyPoint(float(ptsX), float(ptsY), 1) for ptsX, ptsY in pts]\n",
    "    _, des = sift.compute(img, kp)\n",
    "    features = [des[i] for i in range(des.shape[0])]\n",
    "    return features\n",
    "\n",
    "def extract_image_patch_feature(img, pts, patch_size):\n",
    "    \"\"\"\n",
    "    Given an image and some interest points,\n",
    "    create features by flattening patches (of size patch_size by patch_size)\n",
    "    centered around each interest point into 1D vectors.\n",
    "    Args:\n",
    "        img: input image\n",
    "        pts: detected interest points in the previous step\n",
    "        patchSize: an odd number indicate patch size\n",
    "    Return:\n",
    "        features: a list of image patch features (1-d) for each interest point (list of 1D numpy arrays)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "    \"\"\" END YOUR CODE \"\"\"\n",
    "    return features\n",
    "\n",
    "# Here is code for you to test your implementation\n",
    "# SIFT feature descriptors have 128 dimensions\n",
    "# How many dimensions should image patches as features have here?\n",
    "feat_sift = extract_sift_feature(sample_img, pts_sift)\n",
    "print(f\"Total number of SIFT features: {len(feat_sift)}\")\n",
    "print(f\"Dimension of SIFT features: {feat_sift[0].shape}\")\n",
    "feat_patch = extract_image_patch_feature(sample_img, pts_sift, patch_size)\n",
    "print(f\"Total number of patch features: {len(feat_patch)}\")\n",
    "print(f\"Dimension of patch features: {feat_patch[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0f97a-16af-4c0c-978f-6b046453e312",
   "metadata": {},
   "source": [
    "## 1.3. Build visual vocabulary [4 pts]\n",
    "Use k-means clustering to form a visual vocabulary. You are not expected to implement k-means from scratch. You can use sklearn's implementation of k-means clustering: [sklearn k-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "\n",
    "- **get_img_feat()** -- gives you a bunch of 1-D features of an image. This function should return a 2-D array of shape ($n \\times m$) where n is the number of interest points and m is the size of the feature vectors for each interest point\n",
    "\n",
    "- **collect_feat()** -- should collect such features for all images in your dataset and return them as a single 2-D array of features. Remember for each image you are generating up to `n_pts` number of features. Again, you should return a 2-D array of shape ($n \\times m$) where n is the number of interest points (this time across all images) and m is the size of the feature vectors for each interest point\n",
    "\n",
    "- **form_visual_vocab()** -- you will have to cluster the features in to `k` cluster centers. Each center will be represented in the same dimensional space as that of the the feature vector. In the context of this assignment, each cluster will represent a visual \"word\" within our bag of words. You should run k-means with `k=n_clusters` which is provided for you. The general idea of k-means is an iterative algorithm:   \n",
    "    - First, compare each feature vector with the K - cluster centers and assign it the cluster closest to it\n",
    "    - Next, update the cluster center by averaging all the points that were assigned to it\n",
    "    - Repeat this process until you see no change in the K centers (or change very little)\n",
    "\n",
    "\n",
    "**As mentioned previously, you can use sklearn's implementation of this. To find the clusters, you will want to use .fit()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cf974a-361e-4e18-bdae-bcaeb32bbb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Parameters used for (1.3) -- DO NOT CHANGE\n",
    "n_clusters = 100\n",
    "\n",
    "def get_img_feat(img, im_size, n_pts, w_grid, patch_size, pt_type, feat_type):\n",
    "    \"\"\"\n",
    "    Output a 2D array of features for a single image\n",
    "    Args:\n",
    "        img: image from which you want to extract interest points\n",
    "        im_size: size of images (height, width)\n",
    "        n_pts: maximum number of interest points to be extracted\n",
    "        w_grid: width of the small grids\n",
    "        patch_size: an odd number to indicate the patch size\n",
    "        pt_type: 'uniform' or 'sift_keypoint' indicates the interest point sampling method\n",
    "        feat_type: 'sift_descriptor' or 'patch' indicates the feature extraction method\n",
    "    Return:\n",
    "        extract_feat_list: a 2D array of images features for each interest point (n_feats, feat_dim)\n",
    "    \"\"\"\n",
    "    if pt_type == \"uniform\":\n",
    "        int_pts = uniform_sampling(im_size, n_pts, w_grid)\n",
    "    elif pt_type == \"sift_keypoint\":\n",
    "        int_pts = sift_keypoint_sampling(img, n_pts)\n",
    "    else:\n",
    "        assert False, \"pt_type must be either uniform or sift_keypoint\"\n",
    "\n",
    "    if feat_type == \"sift_descriptor\":\n",
    "        extract_feat_list = extract_sift_feature(img, int_pts)\n",
    "    elif feat_type == \"patch\":\n",
    "        extract_feat_list = extract_image_patch_feature(img, int_pts, patch_size)\n",
    "    else:\n",
    "        assert False, \"feat_type must be either sift_descriptor or patch\"\n",
    "\n",
    "    # Convert to 2D array\n",
    "    return np.array(extract_feat_list)\n",
    "\n",
    "def collect_feat(img_list, im_size, n_pts, w_grid, patch_size, pt_type, feat_type):\n",
    "    \"\"\"\n",
    "    Collects extracted features for all images in the list of images\n",
    "    Args:\n",
    "        img_list: list of images (filepaths)\n",
    "        im_size: size of images (height, width)\n",
    "        n_pts: maximum number of interest points to be extracted\n",
    "        w_grid: width of the small grids\n",
    "        patch_size: an odd number to indicate the patch size\n",
    "        pt_type: 'uniform' or 'sift_keypoint' indicates the interest point sampling method\n",
    "        feat_type: 'sift_descriptor' or 'patch' indicates the feature extraction method\n",
    "    Return:\n",
    "        feats: 2D array (# of features, dim of feature)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "    \"\"\" END YOUR CODE \"\"\"\n",
    "    return feats\n",
    "\n",
    "def form_visual_vocab(feats, n_clusters):\n",
    "    \"\"\"\n",
    "    Use the k-means algorithm to find k cluster centers.\n",
    "    Output the k-means model.\n",
    "    Args:\n",
    "        feats: 2D array of features collected from the training data (# of features, dim of feature)\n",
    "        n_cluster: number of clusters in k-means algorithm\n",
    "    Return:\n",
    "        model: fit k-means model\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "    \"\"\" END YOUR CODE \"\"\"\n",
    "    return model\n",
    "\n",
    "# Here is the code for you to test your implementation\n",
    "feats = collect_feat(\n",
    "    train_pos_list, \n",
    "    im_size, \n",
    "    n_pts, \n",
    "    w_grid, \n",
    "    patch_size, \n",
    "    pt_type=\"uniform\", \n",
    "    feat_type=\"sift_descriptor\"\n",
    ")\n",
    "print(f\"feats shape: {feats.shape}\") # should be (total number of extract features, dim of feature)\n",
    "model = form_visual_vocab(feats, n_clusters)\n",
    "centers = model.cluster_centers_\n",
    "print(f\"K-Means centers shape: {centers.shape}\") # should be (n_clusters, dim of feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea358414-5b26-4b0f-a0bd-a033f355c603",
   "metadata": {},
   "source": [
    "## 1.4. Compute histogram representation [4 pts]\n",
    "Compute the histogram representation of each image, with bins defined over the visual words in the vocabulary. These histograms are the visual bag-of-words representations of images that will be used for image classiﬁcation.\n",
    "\n",
    "- In the `get_histogram()` function, you are asked to represent an input image as a histogram over the clusters they are assigned to. Your output should be a 1-D array of the counts of each cluster present in that image\n",
    "- In the `compute_histograms()` function, you are asked to simply collect and put together the histogram representations of all the images in your training dataset. You should aim for another 2D array with shape `(n_imgs, n_clusters)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df7c48-8f4b-4c75-82b2-e7fa443eadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histogram(img, model, n_clusters, im_size, n_pts, w_grid, patch_size, pt_type, feat_type):\n",
    "    \"\"\"\n",
    "    Compute histogram representation for a single image\n",
    "    Args:\n",
    "        img: image from which you want to extract interest points\n",
    "        model: trained k-means model\n",
    "        n_clusters: number of clusters in k-means algorithm\n",
    "        im_size: size of images (height, width)\n",
    "        n_pts: maximum number of interest points to be extracted\n",
    "        w_grid: width of the small grids\n",
    "        patch_size: an odd number to indicate the patch size\n",
    "        pt_type: 'uniform' or 'sift_keypoint' indicates the interest point sampling method\n",
    "        feat_type: 'sift_descriptor' or 'patch' indicates the feature extraction method\n",
    "    Return:\n",
    "        hist: histogram representation (1-d) numpy array for input image\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "    \"\"\" END YOUR CODE \"\"\"\n",
    "    return hist\n",
    "\n",
    "def compute_histograms(img_list, model, n_clusters, im_size, n_pts, w_grid, patch_size, pt_type, feat_type):\n",
    "    \"\"\"\n",
    "    Compute histogram representation for all images in img_list\n",
    "    Args:\n",
    "        train_list: list of images (filepaths)\n",
    "        model: trained k-means model\n",
    "        n_clusters: number of clusters in k-means algorithm\n",
    "        im_size: size of images (height, width)\n",
    "        n_pts: maximum number of interest points to be extracted\n",
    "        w_grid: width of the small grids\n",
    "        patch_size: an odd number to indicate the patch size\n",
    "        pt_type: 'uniform' or 'sift_keypoint' indicates the interest point sampling method\n",
    "        feat_type: 'sift_descriptor' or 'patch' indicates the feature extraction method\n",
    "    Return:\n",
    "        hists: (# of images, n_clusters) array - histograms for each image stacked as rows\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "    \"\"\" END YOUR CODE \"\"\"\n",
    "    return hists\n",
    "\n",
    "# Here is code for you to test your implementation\n",
    "hist = get_histogram(sample_img, model, n_clusters, im_size, n_pts, w_grid, patch_size, pt_type='uniform', feat_type='sift_descriptor')\n",
    "print(f\"Histogram shape: {hist.shape}\") # Should be (n_clusters,)\n",
    "hists = compute_histograms(train_pos_list, model, n_clusters, im_size, n_pts, w_grid, patch_size, pt_type='uniform', feat_type='sift_descriptor')\n",
    "print(f\"Histograms shape: {hists.shape}\") # Should be (# of positive training imgs, n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e8f88-d1ec-41dd-b400-8fd56b9e5559",
   "metadata": {},
   "source": [
    "## 1.5. K nearest neighbor classifier [4 pts]\n",
    "After building the visual vocabulary, we now build an image classiﬁcation model using the nearest neighbors method. Given a new image (test), we first represent it as a histogram using the visual vocabulary and then find the k closest representations in the training set. The test image is assigned the same category as its k nearest neighbors in the training set (using majority voting). Here, we implement the code to create a k-NN classifier and compute accuracy using its predictions. You are not expected to implement this model from scratch and can use `KNeighborsClassifier()` function from `sklearn`. We will eventually test with `k=3` and `k=5`. We will choose to use a different metric than Euclidean distance here. **You should use the `cosine` similarity metric (rather than the default Minkowski with `p=2` which is the same as Euclidean distance).** Refer to the documentation on how to use this metric.\n",
    "\n",
    "Second, you should compute the *testing* accuracy of your KNN model in the `get_accuracy()` function. You can use the `score()` function provided by `sklearn`.\n",
    "\n",
    "Refer: \n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "2. https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e3b36-3028-4135-91bb-fce84180fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def KNN_classifier(train_X, train_y, n_neighbors):\n",
    "    \"\"\"\n",
    "    Return a trained KNN model by fitting the training data\n",
    "    Note: YOU WILL BE USING THE `cosine` METRIC! Refer to documention on how to apply this.\n",
    "    Args:\n",
    "        train_X: a (# of images, n_cluster) array of BoW features for training\n",
    "        train_y: a (# of images) array of class labels for training\n",
    "        n_neighbors: # of neighbors used in KNN\n",
    "    Return:\n",
    "        model: trained KNN classifier model\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "    \"\"\" END YOUR CODE \"\"\"\n",
    "    return model\n",
    "\n",
    "def get_accuracy(test_X, test_y, model):\n",
    "    \"\"\"\n",
    "    Outputs the testing accuracy for the KNN classifier model (decimal)\n",
    "    Args:\n",
    "        test_X: a (# of images, nCluster) array of BoW features for testing data\n",
    "        test_y: a (# of images) array of class label for testing data\n",
    "        model: a trained KNN classifier model to evaluate\n",
    "    Return:\n",
    "        accuracy: testing accuracy of classification prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "    \"\"\" END YOUR CODE \"\"\"\n",
    "    return accuracy\n",
    "\n",
    "# Here is the code for your to test your implementation\n",
    "X = np.array([[0.0, 1.0], [0.0, 2.0], [1.0, 0.0], [2.0, 0.0]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "knn_model = KNN_classifier(X, y, n_neighbors=3)\n",
    "print(f\"Prediction of the vector (3.0, 0.0): {model.predict(np.array([[3.0, 0.0]]))}\") # Should be [1]\n",
    "X_p = np.array([[4.0, 0.0], [10.0, 0.0]])\n",
    "y_p = np.array([0, 1])\n",
    "acc = get_accuracy(X_p, y_p, knn_model)\n",
    "print(f\"Accuracy: {acc}\") # Should be 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e2e12-f63d-49ce-924f-95cf2b21b648",
   "metadata": {},
   "source": [
    "## 1.6. Calculate testing accuracy [4 pts]\n",
    "Using `100 clusters`, try `k=3` and `k=5` for your KNN classifiers respectively. Report the accuracy in the following two 2D tables. You should report the positive accuracy, negative accuracy, and combined (both positive and negative) accuracy from your testing samples. Some of the methods may have poor accuracy. This is expected, so don't worry too much about accuracy. You will get full credit as long as you can correctly implement and reason about the various methods. The highest combined accuracy obtained on one of the set-ups is around 70%.\n",
    "\n",
    "You should:\n",
    "- Train your K-Means and K-NN models using the **training set**\n",
    "- Only compute accuracy using the **testing set**\n",
    "- It's important not to mix the training and testing sets!\n",
    "\n",
    "Note: this can take a bit of time to run. That's normal!\n",
    "\n",
    "---------------------------------------------------------\n",
    "**K=3**\n",
    "|             |  Uniform   | Uniform | Uniform |  SIFT Keypoint  | SIFT Keypoint | SIFT Keypoint |\n",
    "|:------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n",
    "|             | Positive | Negative | Combined|  Positive | Negative | Combined |\n",
    "| SIFT Descriptor |         |         |         |         |         |         |\n",
    "|  Image Patch |         |         |         |         |         |         |\n",
    "\n",
    "---------------------------------------------------------\n",
    "\n",
    "**K=5**\n",
    "\n",
    "|             |  Uniform   | Uniform | Uniform | SIFT Keypoint  | SIFT Keypoint | SIFT Keypoint |\n",
    "|:------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n",
    "|             | Positive | Negative | Combined | Positive | Negative | Combined \n",
    "| SIFT Descriptor |         |         |         |         |         |         |\n",
    "|  Image Patch |         |         |         |         |         |         |\n",
    "\n",
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03614811-a9c3-4d77-838d-8cdeb1de9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Parameters Re-defined for Convenience -- DO NOT CHANGE\n",
    "im_size = (133, 200) # Resize each image to this size\n",
    "n_pts = 200 # Number of points to be sampled\n",
    "w_grid = 5 # Width of the grid used for uniform sampling\n",
    "patch_size = 11 # Each patch will be 11x11 squares\n",
    "n_clusters = 100\n",
    "\n",
    "\"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "\"\"\" END YOUR CODE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca206f96-0e67-47a3-9ff9-79845fa444cd",
   "metadata": {},
   "source": [
    "## Problem 2:  Bayesian Estimation [20 pts]\n",
    "**Note**: You can just use pen and paper to do the computations for this problem. It is also fine to use Python or any other means to do the computations. Please just select PDF pages accordingly and **make sure that it's visible in the PDF**.\n",
    "\n",
    "We wish to classify emails into spam and not spam. We hypothesize that longer emails tend to be more likely spam. Our training data consists of 800 emails labeled as not spam and 400 emails as spam, with the distribution shown in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6172392a-30c9-4c34-bd3c-8da9e5ea658d",
   "metadata": {},
   "source": [
    "|Email Status  |              |                      |    Length (words)    |                      |                    |\n",
    "|:------------:|:------------:|:--------------------:|:--------------------:|:--------------------:|:------------------:|\n",
    "|          | $<100$ words | $100 \\leq 200$ words | $200 \\leq 300$ words | $300 \\leq 400$ words | $400 \\leq $ words  |\n",
    "| Not Spam |     180      |           360        |          140         |          115         |          5         |\n",
    "| Spam     |      15      |          45          |          110         |          80          |         150        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dbfcf3-6e53-46dc-b310-1fbc5dbc5bc8",
   "metadata": {},
   "source": [
    "### Problem **2.1** \n",
    "\n",
    "Consider histogram representations for the likelihoods $P(\\text{number of words}|\\text{spam})$ and $P(\\text{number of words}|\\text{not spam})$. Each histogram has 5 bins, where each bin represents the different ranges of email lengths as shown in the table. Draw an approximate sketch of the histograms in two separate figures. **[3 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36894d3-0f92-4ca4-b0a9-6a5aed76b40a",
   "metadata": {},
   "source": [
    "### Problem **2.2** \n",
    "\n",
    "State the procedure to do maximum likelihood (ML) estimation. Use it to determine **spam** or **not spam** labels for the three bins based on the data above:\n",
    "- $400 \\leq $ words\n",
    "- $100 \\leq 200$ words\n",
    "- $200 \\leq 300$ words\n",
    "\n",
    "**[5 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac02712c-e1f0-49c1-abcc-3a83d969e95b",
   "metadata": {},
   "source": [
    "### Problem **2.3** \n",
    "\n",
    "What is a good prior in this example for $P(spam)$ and $P(not\\ spam)$? **[2 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f329c5d-aa9d-4dc2-a7c5-97ad7f3d4c65",
   "metadata": {},
   "source": [
    "### Problem **2.4** \n",
    "\n",
    "Compute and draw the approximate curves to illustrate the posterior distributions $P(\\text{spam}|\\text{number of words})$ and $P(\\text{not spam}|\\text{number of words})$.\n",
    "\n",
    "**[5 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7823297-1fdb-4571-bb99-1985596fe8ec",
   "metadata": {},
   "source": [
    "### Problem **2.5** \n",
    "\n",
    "State the procedure to do maximum a posteriori (MAP) estimation. Use it to determine **spam** or **not spam** labels for the three bins:\n",
    "- $400 \\leq $ words\n",
    "- $100 \\leq 200$ words\n",
    "- $200 \\leq 300$ words\n",
    "\n",
    "**[5 points]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec980536-1d28-41f5-a20a-b8b925696224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
